{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/lite/guide/inference\n",
    "\n",
    "https://github.com/tensorflow/examples/blob/master/lite/examples/posenet/android/posenet/src/main/java/org/tensorflow/lite/examples/posenet/lib/Posenet.kt\n",
    "\n",
    "# Pose Estimation @ Edge AI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import ipywidgets\n",
    "from base64 import b64decode , b64encode\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorrt\n",
    "# import tensorflow as tf\n",
    "import tflite_runtime.interpreter as tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pathlib.Path(\"/home/unccv/drone_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEIGHT, WIDTH  = input_details[0][\"shape\"][1:3]\n",
    "HEIGHT , WIDTH =353,257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = Image.open(os.path.join(path , \"test_img.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img.resize((256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=os.path.join(path , \"posenet_mobilenet_float_075_1_default_1.tflite\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.expand_dims(test_img.resize((WIDTH ,HEIGHT)), axis=0)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mean , input_std = 127.5  ,127.5\n",
    "input_data = (np.float32(input_data) - input_mean) / input_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "results = np.squeeze(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[None]*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,it in enumerate(BodyPart):\n",
    "    print(idx,it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import numpy as np\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "class BodyPart(enum.IntEnum):\n",
    "    __order__ = \"NOSE LEFT_EYE RIGHT_EYE LEFT_EAR RIGHT_EAR LEFT_SHOULDER RIGHT_SHOULDER LEFT_ELBOW RIGHT_ELBOW LEFT_WRIST RIGHT_WRIST LEFT_HIP RIGHT_HIP LEFT_KNEE RIGHT_KNEE LEFT_ANKLE RIGHT_ANKLE\"\n",
    "    NOSE = 0\n",
    "    LEFT_EYE = 1\n",
    "    RIGHT_EYE= 2\n",
    "    LEFT_EAR= 3\n",
    "    RIGHT_EAR= 4\n",
    "    LEFT_SHOULDER= 5\n",
    "    RIGHT_SHOULDER = 6\n",
    "    LEFT_ELBOW = 7\n",
    "    RIGHT_ELBOW = 8\n",
    "    LEFT_WRIST= 9\n",
    "    RIGHT_WRIST= 10\n",
    "    LEFT_HIP= 11\n",
    "    RIGHT_HIP= 12\n",
    "    LEFT_KNEE= 13\n",
    "    RIGHT_KNEE = 14\n",
    "    LEFT_ANKLE = 15\n",
    "    RIGHT_ANKLE = 16\n",
    "\n",
    "class Position:\n",
    "    def __init__(self, x=0,y=0):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "class KeyPoint:\n",
    "    def __init__(self,bodypart = BodyPart.NOSE, position = Position() , score=0.0 ):\n",
    "        self.bodyPart = bodypart\n",
    "        self.position = position\n",
    "        self.score = score\n",
    "\n",
    "class Person:\n",
    "    def __init__(self,keypoints = [] , score=0.0):\n",
    "        self.keyPoints = keypoints\n",
    "        self.score = score\n",
    "\n",
    "class Posenet:\n",
    "\n",
    "    def __init__(self,model_path=\"posenet_model.tflite\"):\n",
    "        self.lastInferenceTimeNanos = -1\n",
    "        self.interpreter = None\n",
    "        self.gpuDelegate = None\n",
    "        self.model_path = model_path\n",
    "        self.NUM_LITE_THREADS  = 4\n",
    "\n",
    "\n",
    "    def getInterpreter(self):\n",
    "        if self.interpreter is not None:\n",
    "            return self.interpreter\n",
    "        interpreter = tflite.Interpreter(model_path=self.model_path , num_threads = self.NUM_LITE_THREADS)\n",
    "        interpreter.allocate_tensors()\n",
    "        self.input_details = interpreter.get_input_details()\n",
    "        self.output_details = interpreter.get_output_details()\n",
    "        self.interpreter = interpreter\n",
    "        return interpreter\n",
    "\n",
    "    def close(self):\n",
    "        self.interpreter.close()\n",
    "        self.interpreter = None\n",
    "\n",
    "    def sigmoid(self , x):\n",
    "        return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "    def getKeyPointLocations(self, heatmaps):\n",
    "        height , width , numKeyPoints = heatmaps.shape\n",
    "        keypointPositions = [None]*numKeyPoints\n",
    "        for keypoint in range(numKeyPoints):\n",
    "            maxVal  = heatmaps[0][0][keypoint ]\n",
    "            maxRow  , maxCol = 0,0\n",
    "            for row in range(height):\n",
    "                for col in range(width):\n",
    "                     if (heatmaps[row][col][keypoint] > maxVal):\n",
    "                         maxVal = heatmaps[row][col][keypoint]\n",
    "                         maxRow = row\n",
    "                         maxCol = col\n",
    "\n",
    "            keypointPositions[keypoint] = (maxRow, maxCol)\n",
    "\n",
    "        return keypointPositions\n",
    "\n",
    "    def getConfidenceScores(self,heatmaps ,offsets,keypointPositions , height , width, HEIGHT , WIDTH):\n",
    "        numKeyPoints = len(keypointPositions)\n",
    "        xCoords = np.zeros(numKeyPoints)\n",
    "        yCoords = np.zeros(numKeyPoints)\n",
    "        confidenceScores  = np.zeros(numKeyPoints)\n",
    "\n",
    "        for idx ,position in enumerate(keypointPositions):\n",
    "            positionY  = keypointPositions[idx][0]\n",
    "            positionX = keypointPositions[idx][1]\n",
    "            yCoords[idx] = int( position[0] / float(height - 1) * HEIGHT + offsets[positionY][positionX][idx])\n",
    "            xCoords[idx] = int( position[1] / float(width - 1) * WIDTH + offsets[positionY][positionX][idx])\n",
    "            confidenceScores[idx] = self.sigmoid(heatmaps[positionY][positionX][idx])\n",
    "\n",
    "        return xCoords , yCoords , confidenceScores\n",
    "\n",
    "    def getPersonDetails(self , numKeyPoints , xCoords , yCoords,confidenceScores):\n",
    "        person = Person()\n",
    "        keypointList = []\n",
    "        totalScore = 0\n",
    "        for idx,it in enumerate(BodyPart):\n",
    "            kp = KeyPoint()\n",
    "            kp.bodyPart = it\n",
    "            kp.position = Position(xCoords[idx],yCoords[idx]) \n",
    "            kp.score  = confidenceScores[idx]\n",
    "            keypointList.append(kp)\n",
    "            \n",
    "            totalScore += confidenceScores[idx]\n",
    "\n",
    "        person.keyPoints = keypointList\n",
    "        person.score = totalScore / numKeyPoints\n",
    "        return person\n",
    "\n",
    "    def estimateSinglePose(self, image):\n",
    "        self.getInterpreter()\n",
    "        \n",
    "        HEIGHT, WIDTH  = self.input_details[0][\"shape\"][1:3]\n",
    "        input_data = np.expand_dims(image.resize((WIDTH ,HEIGHT)), axis=0)\n",
    "        input_mean , input_std = 127.5  ,127.5\n",
    "        input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
    "        self.interpreter.invoke()\n",
    "\n",
    "        heatmaps  = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "        heatmaps  = np.squeeze(heatmaps)\n",
    "\n",
    "        offsets   = self.interpreter.get_tensor(self.output_details[1]['index'])\n",
    "        offsets   = np.squeeze(offsets )\n",
    "\n",
    "        height , width , numKeyPoints = heatmaps.shape\n",
    "\n",
    "        keypointPositions = self.getKeyPointLocations(heatmaps )\n",
    "        \n",
    "        xCoords , yCoords , confidenceScores = (self.getConfidenceScores(heatmaps\n",
    "                                                    , offsets\n",
    "                                                    ,keypointPositions\n",
    "                                                    , height\n",
    "                                                    , width\n",
    "                                                    , HEIGHT\n",
    "                                                    , WIDTH))\n",
    "        \n",
    "        \n",
    "        print(xCoords , yCoords,confidenceScores)\n",
    "        \n",
    "        person = self.getPersonDetails( numKeyPoints , xCoords , yCoords,confidenceScores)\n",
    "        return person\n",
    "    \n",
    "    def getDrawnImage(self, image):\n",
    "        person = self.estimateSinglePose(image)\n",
    "        out_img = np.array(image.resize((WIDTH ,HEIGHT)))\n",
    "        for keypoint in person.keyPoints:\n",
    "            out_img = cv2.circle( out_img , (int(keypoint.position.x) , int(keypoint.position.y)) , 10 , (42, 157, 143))\n",
    "        return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = Posenet(os.path.join(path , \"posenet_mobilenet_float_075_1_default_1.tflite\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = pnet.estimateSinglePose(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.keyPoints[0].bodyPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.keyPoints[0].position.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.keyPoints[14].bodyPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.keyPoints[14].position.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StickMan:\n",
    "    \n",
    "    def lineBetweenPoints(self,image,pointA , pointB):\n",
    "        return cv2.line(image \n",
    "                        , (int(pointA.position.x) , int(pointA.position.y)) \n",
    "                        , (int(pointB.position.x) , int(pointB.position.y))\n",
    "                       , (42, 157, 143) , 5)\n",
    "        \n",
    "    def draw(self ,image , keypoints):\n",
    "        out_img = np.array(image.resize((WIDTH ,HEIGHT)))\n",
    "        for keypoint in person.keyPoints:\n",
    "            out_img = cv2.circle( out_img , (int(keypoint.position.x) , int(keypoint.position.y)) , 5 , (42, 157, 143) , -1)\n",
    "        \n",
    "        \n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.LEFT_WRIST)] , keypoints[int(BodyPart.LEFT_ELBOW)])\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.LEFT_ELBOW)] , keypoints[int(BodyPart.LEFT_SHOULDER)])\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.LEFT_SHOULDER)] , keypoints[int(BodyPart.RIGHT_SHOULDER)])\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.RIGHT_SHOULDER)] , keypoints[int(BodyPart.RIGHT_ELBOW)])\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.RIGHT_ELBOW)] , keypoints[int(BodyPart.RIGHT_WRIST)])\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.RIGHT_ELBOW)] , keypoints[int(BodyPart.RIGHT_WRIST)])\n",
    "        \n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.LEFT_HIP)] , keypoints[int(BodyPart.LEFT_KNEE)])\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.LEFT_KNEE)] , keypoints[int(BodyPart.LEFT_ANKLE)])\n",
    "        \n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.RIGHT_HIP)] , keypoints[int(BodyPart.RIGHT_KNEE)])\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.RIGHT_KNEE)] , keypoints[int(BodyPart.RIGHT_ANKLE)])\n",
    "        \n",
    "        return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(StickMan().draw(test_img  ,person.keyPoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(pnet.getDrawnImage(test_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from jetcam.usb_camera import USBCamera\n",
    "import cv2\n",
    "from jetcam.csi_camera import CSICamera\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "\n",
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "#camera = USBCamera(width=WIDTH, height=HEIGHT, capture_fps=30)\n",
    "camera = CSICamera(width=WIDTH, height=HEIGHT, capture_fps=10)\n",
    "\n",
    "#camera.running = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "HEIGHT , WIDTH =353,257\n",
    "\n",
    "def gstreamer_pipeline (capture_width=1640, capture_height=1232 , display_width=244, \n",
    "     display_height=244, framerate=21, flip_method=2) :   \n",
    "     return ('nvarguscamerasrc ! ' \n",
    "    'video/x-raw(memory:NVMM), '\n",
    "    'width=(int)%d, height=(int)%d, '\n",
    "    'format=(string)NV12, framerate=(fraction)%d/1 ! '\n",
    "    'nvvidconv flip-method=%d ! '\n",
    "    'video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! '\n",
    "    'videoconvert ! video/x-raw,format=BGR !'\n",
    "    #'videorate ! video/x-raw,framerate=5/1 !'\n",
    "    #'appsink wait-on-eos=false max-buffers=2 drop=True'\n",
    "    'appsink'% (capture_width,capture_height,framerate,flip_method,display_width,display_height))\n",
    "    \n",
    "camera2 = cv2.VideoCapture(gstreamer_pipeline(display_width = WIDTH , display_height=HEIGHT))#, cv2.CAP_GSTREAMER)\n",
    "#camera2.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "#retval, im = camera2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           1971         869         513          10         588         997\r\n",
      "Swap:          5081         398        4683\r\n"
     ]
    }
   ],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera2.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam_read():\n",
    "    #return camera2.read()#camera2.retrieve(camera2.grab())\n",
    "    img = camera2.read()[1]\n",
    "    #img = camera.value\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return _ , img\n",
    "    #return _,camera.value\n",
    "    #return _, camera.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ret,img = cam_read()\n",
    "ret,img = cam_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stick_man = StickMan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ipywidgets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7b32d0a783c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_widget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mipywidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jpg'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ipywidgets' is not defined"
     ]
    }
   ],
   "source": [
    "image_widget = ipywidgets.Image(format='jpg' , height=256 ,width=256)\n",
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        image_widget.value =  cv2.imencode('.jpg',cam_read()[1])[1].tobytes()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Breaking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        img =cam_read()[1]\n",
    "        pil_img = Image.fromarray(img)\n",
    "        person = pnet.estimateSinglePose(pil_img)\n",
    "        img = stick_man.draw(pil_img,person.keyPoints)\n",
    "        image_widget.value =  cv2.imencode('.jpg',img)[1].tobytes()\n",
    "        clear_output(wait=True)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Breaking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.expand_dims(test_img.resize((WIDTH ,HEIGHT)), axis=0)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mean , input_std = 127.5  ,127.5\n",
    "input_data = (np.float32(input_data) - input_mean) / input_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "results = np.squeeze(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/60032705/how-to-parse-the-heatmap-output-for-the-pose-estimation-tflite-model\n",
    "\n",
    "https://github.com/google-coral/project-posenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(heatmaps, offsets, output_stride=32):\n",
    "        scores = sigmoid(heatmaps)\n",
    "        num_keypoints = scores.shape[2]\n",
    "        heatmap_positions = []\n",
    "        offset_vectors = []\n",
    "        confidences = []\n",
    "        for ki in range(0, num_keypoints ):\n",
    "            x,y = np.unravel_index(np.argmax(scores[:,:,ki]), scores[:,:,ki].shape)\n",
    "            confidences.append(scores[x,y,ki])\n",
    "            offset_vector = (offsets[y,x,ki], offsets[y,x,num_keypoints+ki])\n",
    "            heatmap_positions.append((x,y))\n",
    "            offset_vectors.append(offset_vector)\n",
    "        image_positions = np.add(np.array(heatmap_positions) * output_stride, offset_vectors)\n",
    "        keypoints = [KeyPoint(i, pos, confidences[i]) for i, pos in enumerate(image_positions)]\n",
    "        return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the JSON file which describes the human pose task.  This is in COCO format, it is the category descriptor pulled from the annotations file.  We modify the COCO category slightly, to add a neck keypoint.  We will use this task description JSON to create a topology tensor, which is an intermediate data structure that describes the part linkages, as well as which channels in the part affinity field each linkage corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import trt_pose.coco\n",
    "\n",
    "with open('human_pose.json', 'r') as f:\n",
    "    human_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch2trt\n",
    "import torch\n",
    "OPTIMIZED_MODEL = 'resnet18_baseline_att_224x224_A_epoch_249_trt.pth'\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "data = torch.zeros((1, 3, HEIGHT, WIDTH)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch2trt import TRTModule\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a function that will preprocess the image, which is originally in BGR8 / HWC format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def preprocess(image):\n",
    "    global device\n",
    "    device = torch.device('cuda')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define two callable classes that will be used to parse the objects from the neural network, as well as draw the parsed objects on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trt_pose.draw_objects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "parse_objects = ParseObjects(topology)\n",
    "draw_objects = DrawObjects(topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you're using NVIDIA Jetson, you can use the [jetcam](https://github.com/NVIDIA-AI-IOT/jetcam) package to create an easy to use camera that will produce images in BGR8/HWC format.\n",
    "\n",
    "If you're not on Jetson, you may need to adapt the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a widget which will be used to display the camera feed with visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define the main execution loop.  This will perform the following steps\n",
    "\n",
    "1.  Preprocess the camera image\n",
    "2.  Execute the neural network\n",
    "3.  Parse the objects from the neural network output\n",
    "4.  Draw the objects onto the camera image\n",
    "5.  Convert the image to JPEG format and stream to the display widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(image):\n",
    "    #image = change['new']\n",
    "    data = preprocess(image)\n",
    "    cmap, paf = model_trt(data)\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf)#, cmap_threshold=0.15, link_threshold=0.15)\n",
    "    draw_objects(image, counts, objects, peaks)\n",
    "#     image_w.value = bgr8_to_jpeg(image[:, ::-1, :])\n",
    "#     #return bgr8_to_jpeg(image[:, ::-1, :])\n",
    "#     return show_local_img(image[:, ::-1, :] , 256,256)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import time\n",
    "import PIL.Image\n",
    "outpath = os.path.abspath(os.path.join(\".\" , \"..\" , \"..\" , \"..\" , \"..\" , \"drone_project\" , \"outdump\"))\n",
    "outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_widget = ipywidgets.Image(format='jpg' , height=256 ,width=256)\n",
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 50]\n",
    "i = 0\n",
    "try:\n",
    "    t1 = time.time()\n",
    "    while(True):\n",
    "        \n",
    "        # Capture frame-by-frame\n",
    "        #_ , frame = camera2.read()\n",
    "        #ret, frame = cam_read()\n",
    "        camera2.grab()\n",
    "        t2 = time.time()\n",
    "        \n",
    "        \n",
    "            \n",
    "        # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "        # to display the image\n",
    "        #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        #exec_2(frame)\n",
    "        if int(t2 - t1) > 2:\n",
    "            ret, frame = cam_read()\n",
    "            if not ret:\n",
    "                print(\"Failed\")\n",
    "                break\n",
    "            t1 = t2\n",
    "            #frame = execute(frame)\n",
    "            image_widget.value = cv2.imencode('.jpg', frame , encode_param)[1].tobytes()\n",
    "            #cv2.imwrite(os.path.join(outpath , f\"{i}.jpg\") , frame)\n",
    "            i += 1\n",
    "        #image_widget.value = cv2.imencode('.jpg', frame , encode_param)[1].tobytes()\n",
    "        #frame = execute(frame)\n",
    "        #show_local_img(frame , 256, 256)\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(f\"waiting {t2 - t1}\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    #cam.release()\n",
    "    print(\"Stream stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_2({'new': camera.value})\n",
    "camera.observe(exec_2, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(image):\n",
    "    #image = change['new']\n",
    "    data = preprocess(image)\n",
    "    cmap, paf = model_trt(data)\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf)#, cmap_threshold=0.15, link_threshold=0.15)\n",
    "    draw_objects(image, counts, objects, peaks)\n",
    "#     image_w.value = bgr8_to_jpeg(image[:, ::-1, :])\n",
    "#     #return bgr8_to_jpeg(image[:, ::-1, :])\n",
    "#     return show_local_img(image[:, ::-1, :] , 256,256)\n",
    "    return draw_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call the cell below it will execute the function once on the current camera frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the cell below to attach the execution function to the camera's internal value.  This will cause the execute function to be called whenever a new camera frame is received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the cell below to unattach the camera frame callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
