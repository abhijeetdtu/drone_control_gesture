{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drone Control Using Gestures\n",
    "\n",
    "\n",
    "### Setting Up\n",
    "This is the main file and should be run on the Jetson Nano.\n",
    "To be able to tunnel jupyter notebook from Nano to your laptop follow [this guide](https://www.digitalocean.com/community/tutorials/how-to-install-run-connect-to-jupyter-notebook-on-remote-server)\n",
    "\n",
    "We are doing this in Jupyter Notebook to be able to stream the camera input and our model's outputs to our laptop. Make sure both jetson nano and your laptop are on the same wifi network. This will be important specially for commnuicating gestures between the two.\n",
    "\n",
    "If you haven't wiped out the Nano, everything should already be installed. But if otherwise\n",
    "\n",
    "- Install Python packages \n",
    "    - cv2 - version 4.1.1\n",
    "    - PIL - version 6.2.2\n",
    "    - numpy - version 1.17.1\n",
    "    \n",
    "- Build and Install Tensorflow Lite Runtime 2.4.0\n",
    "    - https://qengineering.eu/install-tensorflow-2-lite-on-jetson-nano.html\n",
    "    - This might be a pain but a cruicial step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import socket\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "import io\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import ipywidgets\n",
    "from base64 import b64decode , b64encode\n",
    "import cv2\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/python3.6/dist-packages/cv2/python-3.6/cv2.cpython-36m-aarch64-linux-gnu.so'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1.1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.2.2'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIL.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.17.1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating GStreamer Pipeline\n",
    "\n",
    "We are using GStreamer. Which is a pipeline-based multimedia\n",
    "framework that links together a wide variety of media processing systems to complete\n",
    "complex workflows. For instance, GStreamer can be used to build a system that reads\n",
    "files in one format, processes them, and exports them in another. The formats and\n",
    "processes can be changed in a plug and play fashion.\n",
    "\n",
    "[Note]\n",
    "\n",
    "NVIDIA Gstreamer is pre-installed on the jetson Nano.\n",
    "\n",
    "To test if its available and working as expected, you may run the following from a ssh terminal onto the Jetson Nano\n",
    "\n",
    "`gst-launch-1.0 nvarguscamerasrc ! 'video/x-raw(memory:NVMM), width=3280, height=2464, format=NV12, framerate=30/1' ! fakesink`\n",
    "\n",
    "[End Note]\n",
    "\n",
    "\n",
    "We are using Gstreamer for 2 broad things\n",
    "\n",
    "1. **Moving Image Frame from GPU Cache to Shared Memory**\n",
    "\n",
    "    Image frames once captured from the camera are moved into the GPU cache. For our processing we moved it to Shared System memory to allow it to be read by our python programs\n",
    "\n",
    "\n",
    "2. **Image Pre-processing**\n",
    "    \n",
    "    a. We then pre-processed the image stream by resizing them to the size needed by the machine learning model.\n",
    "    \n",
    "    b. We also added steps for flipping the image and adjusting the frame rate.\n",
    "    \n",
    "    c. Frame rate was adjusted because the models we were using were not meant for high framerates that the camera supported like 60fps or 120fps.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/6872080/118668202-a84ba400-b7c2-11eb-9c81-8e551706ab62.png)\n",
    "\n",
    "\n",
    "More on GStreamer deveopment guide is [here](https://docs.nvidia.com/jetson/l4t/index.html#page/Tegra%20Linux%20Driver%20Package%20Development%20Guide/accelerated_gstreamer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SRC_WIDTH ,SRC_HEIGHT  =1280,720\n",
    "\n",
    "def gstreamer_pipeline (capture_width=1280, capture_height=720 , display_width=640, \n",
    "     display_height=480, framerate=21, flip_method=2) :   \n",
    "     return f\"\"\"nvarguscamerasrc ! \n",
    "     video/x-raw(memory:NVMM),width=(int){capture_width}, height=(int){capture_height}, format=(string)NV12, framerate=(fraction){framerate}/1 !\n",
    "     nvvidconv  flip-method={flip_method} ! video/x-raw,width=(int){display_width}, height=(int){display_height},  format=BGRx ! \n",
    "     videoconvert ! video/x-raw,format=(string)BGR !\n",
    "     appsink wait-on-eos=false max-buffers=2 drop=True\n",
    "     \"\"\"\n",
    "    #\n",
    "    \n",
    "camera2 = cv2.VideoCapture(gstreamer_pipeline(display_width = SRC_WIDTH , display_height=SRC_HEIGHT))#, cv2.CAP_GSTREAMER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam_read():\n",
    "    img = camera2.read()[1]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return _ , img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Test\n",
    "\n",
    "To camera from within Jupyter Notebook, we will leverage *Ipywidgets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ad3c24dbff45ae993d6d934bdbf2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpg', height='256', width='256')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_widget = ipywidgets.Image(format='jpg' , height=256 ,width=256)\n",
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will create an infinite loop and stream images from the camera to the container created above. To exit the loop use\n",
    "\n",
    "*kernel -> interrupt* option from the toolbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        image_widget.value =  cv2.imencode('.jpg',cam_read()[1])[1].tobytes()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Breaking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pose Estimation Model\n",
    "\n",
    "We are leveraging *Tensorflow lite* for our pose estimation model.\n",
    "\n",
    "Beauty of these *lite* models are that they are quite compressed in size and despite that resonably efficient.\n",
    "\n",
    "We are going to use `wget` to download the model and place it in correct directory.\n",
    "\n",
    "Alternatively - \n",
    "\n",
    "You can download the model from [here](http://59.36.11.51/dataset/workspace/mindspore_dataset/mslite/models/hiai/posenet_mobilenet_float_075_1_default_1.tflite)\n",
    "and place it in `/home/unccv/drone_project` directory. \n",
    "\n",
    "Or you can place it anywhere and just make sure the path below is updated to reflect the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflite_runtime.interpreter as tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflite_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_runtime.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pathlib.Path(\"/home/unccv/drone_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-18 10:32:23--  http://59.36.11.51/dataset/workspace/mindspore_dataset/mslite/models/hiai/posenet_mobilenet_float_075_1_default_1.tflite\n",
      "Connecting to 59.36.11.51:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5048148 (4.8M)\n",
      "Saving to: ‘test_model_05.tflite’\n",
      "\n",
      "                    100%[===================>]   4.81M  1.12MB/s    in 4.5s    \n",
      "\n",
      "2021-05-18 10:32:28 (1.07 MB/s) - ‘test_model_05.tflite’ saved [5048148/5048148]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O test_model_05.tflite http://59.36.11.51/dataset/workspace/mindspore_dataset/mslite/models/hiai/posenet_mobilenet_float_075_1_default_1.tflite -P /home/unccv/drone_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpreter = tflite.Interpreter(model_path=os.path.join(path , \"posenet_mobilenet_float_075_1_default_1.tflite\"))\n",
    "interpreter = tflite.Interpreter(model_path=os.path.join(path , \"test_model_05.tflite\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing loaded Model\n",
    "\n",
    "Below we run a simple test to see if the model loaded correctly. We also try and see what `inputs` and `outputs` model has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'sub_2',\n",
       "  'index': 97,\n",
       "  'shape': array([  1, 353, 257,   3], dtype=int32),\n",
       "  'shape_signature': array([  1, 353, 257,   3], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that the `shape_signature` field is valued as `[1 , 353 , 257, 3]` , therefore the model expects us to pass a batch of 353x257 images with `rgb` channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'float_heatmaps',\n",
       "  'index': 93,\n",
       "  'shape': array([ 1, 23, 17, 17], dtype=int32),\n",
       "  'shape_signature': array([ 1, 23, 17, 17], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'float_short_offsets',\n",
       "  'index': 96,\n",
       "  'shape': array([ 1, 23, 17, 34], dtype=int32),\n",
       "  'shape_signature': array([ 1, 23, 17, 34], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'float_mid_offsets',\n",
       "  'index': 94,\n",
       "  'shape': array([ 1, 23, 17, 64], dtype=int32),\n",
       "  'shape_signature': array([ 1, 23, 17, 64], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'float_segments',\n",
       "  'index': 95,\n",
       "  'shape': array([ 1, 23, 17,  1], dtype=int32),\n",
       "  'shape_signature': array([ 1, 23, 17,  1], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to make sense of all that jargon you will have to understand how this model really works.\n",
    "\n",
    "[Here](https://blog.tensorflow.org/2019/08/track-human-poses-in-real-time-on-android-tensorflow-lite.html) is a blog-post from `Tensorflow Lite`. We will port their code into python below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT, WIDTH  = input_details[0][\"shape\"][1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n",
      "353\n"
     ]
    }
   ],
   "source": [
    "print(WIDTH);print(HEIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some helper classes that will be used with the pose estimation model.\n",
    "\n",
    "1. `BodyPart` class to be able to interpret the outputs of the Model\n",
    "2. `Position` class - represents the x,y position of identified keypoints.\n",
    "3. `Keypoint` class- represents the identified keypoint like elbow or nose etc\n",
    "4. `Person` class- stores all the keypoints of the identified person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class BodyPart(enum.IntEnum):\n",
    "    __order__ = \"NOSE LEFT_EYE RIGHT_EYE LEFT_EAR RIGHT_EAR LEFT_SHOULDER RIGHT_SHOULDER LEFT_ELBOW RIGHT_ELBOW LEFT_WRIST RIGHT_WRIST LEFT_HIP RIGHT_HIP LEFT_KNEE RIGHT_KNEE LEFT_ANKLE RIGHT_ANKLE\"\n",
    "    NOSE = 0\n",
    "    LEFT_EYE = 1\n",
    "    RIGHT_EYE= 2\n",
    "    LEFT_EAR= 3\n",
    "    RIGHT_EAR= 4\n",
    "    LEFT_SHOULDER= 5\n",
    "    RIGHT_SHOULDER = 6\n",
    "    LEFT_ELBOW = 7\n",
    "    RIGHT_ELBOW = 8\n",
    "    LEFT_WRIST= 9\n",
    "    RIGHT_WRIST= 10\n",
    "    LEFT_HIP= 11\n",
    "    RIGHT_HIP= 12\n",
    "    LEFT_KNEE= 13\n",
    "    RIGHT_KNEE = 14\n",
    "    LEFT_ANKLE = 15\n",
    "    RIGHT_ANKLE = 16\n",
    "\n",
    "class Position:\n",
    "    def __init__(self, x=0,y=0):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "class KeyPoint:\n",
    "    def __init__(self,bodypart = BodyPart.NOSE, position = Position() , score=0.0 ):\n",
    "        self.bodyPart = bodypart\n",
    "        self.position = position\n",
    "        self.score = score\n",
    "    \n",
    "    def toJson(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__)\n",
    "    \n",
    "class Person:\n",
    "    def __init__(self,keypoints = [] , score=0.0 , bodyScore=0.0):\n",
    "        self.keyPoints = keypoints\n",
    "        self.score = score\n",
    "        self.bodyScore = bodyScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the core logic for `Person Decoding` the output by the model. This is based on the blogpost above. \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/6872080/118673031-b00d4780-b7c6-11eb-958a-28441874fcfb.png\" width=\"50%\"/>\n",
    "\n",
    "\n",
    "#### Posenet Paper is [here](https://arxiv.org/abs/1803.08225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Posenet:\n",
    "\n",
    "    def __init__(self,model_path=\"posenet_model.tflite\"):\n",
    "        self.lastInferenceTimeNanos = -1\n",
    "        self.interpreter = None\n",
    "        self.gpuDelegate = None\n",
    "        self.model_path = model_path\n",
    "        self.NUM_LITE_THREADS  = 4\n",
    "\n",
    "\n",
    "    def getInterpreter(self):\n",
    "        if self.interpreter is not None:\n",
    "            return self.interpreter\n",
    "        interpreter = tflite.Interpreter(model_path=self.model_path , num_threads = self.NUM_LITE_THREADS)\n",
    "        interpreter.allocate_tensors()\n",
    "        self.input_details = interpreter.get_input_details()\n",
    "        self.output_details = interpreter.get_output_details()\n",
    "        self.interpreter = interpreter\n",
    "        return interpreter\n",
    "\n",
    "    def close(self):\n",
    "        self.interpreter.close()\n",
    "        self.interpreter = None\n",
    "\n",
    "    def sigmoid(self , x):\n",
    "        return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "    def getKeyPointLocations(self, heatmaps):\n",
    "        height , width , numKeyPoints = heatmaps.shape\n",
    "        keypointPositions = [None]*numKeyPoints\n",
    "        for keypoint in range(numKeyPoints):\n",
    "            maxVal  = heatmaps[0][0][keypoint ]\n",
    "            maxRow  , maxCol = 0,0\n",
    "            for row in range(height):\n",
    "                for col in range(width):\n",
    "                     if (heatmaps[row][col][keypoint] > maxVal):\n",
    "                         maxVal = heatmaps[row][col][keypoint]\n",
    "                         maxRow = row\n",
    "                         maxCol = col\n",
    "\n",
    "            keypointPositions[keypoint] = (maxRow, maxCol)\n",
    "\n",
    "        return keypointPositions\n",
    "\n",
    "    def getConfidenceScores(self,heatmaps ,offsets,keypointPositions , height , width, HEIGHT , WIDTH):\n",
    "        numKeyPoints = len(keypointPositions)\n",
    "        xCoords = np.zeros(numKeyPoints)\n",
    "        yCoords = np.zeros(numKeyPoints)\n",
    "        confidenceScores  = np.zeros(numKeyPoints)\n",
    "\n",
    "        for idx ,position in enumerate(keypointPositions):\n",
    "            positionY  = keypointPositions[idx][0]\n",
    "            positionX = keypointPositions[idx][1]\n",
    "            yCoords[idx] = int( position[0] / float(height - 1) * HEIGHT + offsets[positionY][positionX][idx])\n",
    "            xCoords[idx] = int( position[1] / float(width - 1) * WIDTH + offsets[positionY][positionX][idx + numKeyPoints])\n",
    "            confidenceScores[idx] = self.sigmoid(heatmaps[positionY][positionX][idx])\n",
    "\n",
    "        return xCoords , yCoords , confidenceScores\n",
    "\n",
    "    def getPersonDetails(self , numKeyPoints , xCoords , yCoords,confidenceScores):\n",
    "        person = Person()\n",
    "        keypointList = []\n",
    "        totalScore = 0\n",
    "        bodyScore = 0\n",
    "        for idx,it in enumerate(BodyPart):\n",
    "            kp = KeyPoint()\n",
    "            kp.bodyPart = it\n",
    "            kp.position = Position(xCoords[idx],yCoords[idx]) \n",
    "            kp.score  = confidenceScores[idx]\n",
    "            keypointList.append(kp)\n",
    "            \n",
    "            if idx > 4:\n",
    "                bodyScore += confidenceScores[idx]\n",
    "            totalScore += confidenceScores[idx]\n",
    "\n",
    "        person.keyPoints = keypointList\n",
    "        person.score = totalScore / numKeyPoints\n",
    "        #print(bodyScore)\n",
    "        person.bodyScore = bodyScore / (numKeyPoints - 5.0)\n",
    "        return person\n",
    "\n",
    "    def estimateSinglePose(self, image):\n",
    "        self.getInterpreter()\n",
    "        \n",
    "        HEIGHT, WIDTH  = self.input_details[0][\"shape\"][1:3]\n",
    "        input_data = np.expand_dims(image.resize((WIDTH ,HEIGHT) , Image.HAMMING), axis=0)\n",
    "        input_mean , input_std = 127.5  ,127.5\n",
    "        input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
    "        self.interpreter.invoke()\n",
    "\n",
    "        heatmaps  = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "        heatmaps  = np.squeeze(heatmaps)\n",
    "\n",
    "        offsets   = self.interpreter.get_tensor(self.output_details[1]['index'])\n",
    "        offsets   = np.squeeze(offsets )\n",
    "\n",
    "        height , width , numKeyPoints = heatmaps.shape\n",
    "\n",
    "        keypointPositions = self.getKeyPointLocations(heatmaps )\n",
    "        \n",
    "        xCoords , yCoords , confidenceScores = (self.getConfidenceScores(heatmaps\n",
    "                                                    , offsets\n",
    "                                                    ,keypointPositions\n",
    "                                                    , height\n",
    "                                                    , width\n",
    "                                                    , HEIGHT\n",
    "                                                    , WIDTH))\n",
    "        \n",
    "        \n",
    "        #print(xCoords , yCoords,confidenceScores)\n",
    "        \n",
    "        person = self.getPersonDetails( numKeyPoints , xCoords , yCoords,confidenceScores)\n",
    "        return person\n",
    "    \n",
    "    def getDrawnImage(self, image):\n",
    "        person = self.estimateSinglePose(image)\n",
    "        out_img = np.array(image.resize((WIDTH ,HEIGHT)))\n",
    "        for keypoint in person.keyPoints:\n",
    "            out_img = cv2.circle( out_img , (int(keypoint.position.x) , int(keypoint.position.y)) , 10 , (42, 157, 143))\n",
    "        return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pnet = Posenet(os.path.join(path , \"posenet_mobilenet_float_075_1_default_1.tflite\"))\n",
    "pnet = Posenet(os.path.join(path , \"test_model_05.tflite\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing Estimated Positions\n",
    "\n",
    "Once we have extraced out the positions. We want to add them back to the input image and a stick figure. For this we used a confidence threshold of 50%. If the model is not really sure about the point, we don't draw it on the image.\n",
    "\n",
    "To get a stick figure we use `OpenCv`'s `line` function to draw lines between the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StickMan:\n",
    "    \n",
    "    def lineBetweenPoints(self,image,pointA , pointB):\n",
    "        if pointA.score > 0.5 and pointB.score > 0.5:\n",
    "            return cv2.line(image \n",
    "                            , (int(pointA.position.x) , int(pointA.position.y)) \n",
    "                            , (int(pointB.position.x) , int(pointB.position.y))\n",
    "                           , (42, 157, 143) , 2)\n",
    "        return image\n",
    "    \n",
    "    def get_val(self , key_point):\n",
    "        return key_point.position if key_point.score > 0.5 else Position(np.nan,np.nan)\n",
    "    \n",
    "    def get_bounding_box(self,person):\n",
    "        keypoints = person.keyPoints\n",
    "        xmin = self.get_val(keypoints[int(BodyPart.LEFT_WRIST)]).x\n",
    "        xmax = self.get_val(keypoints[int(BodyPart.RIGHT_WRIST)]).x\n",
    "        ymax = self.get_val(keypoints[int(BodyPart.RIGHT_ANKLE)]).y\n",
    "        ymin = self.get_val(keypoints[int(BodyPart.NOSE)]).y\n",
    "        return [xmin , ymin , xmax , ymax]\n",
    "        \n",
    "    def draw(self ,image , person , direction=None):\n",
    "        keypoints = person.keyPoints\n",
    "        #print(keypoints)\n",
    "        out_img = np.array(image.resize((WIDTH ,HEIGHT)))\n",
    "        for keypoint in person.keyPoints:\n",
    "            if keypoint.score > 0.5:\n",
    "                out_img = cv2.circle( out_img , (int(keypoint.position.x) , int(keypoint.position.y)) , 5 , (251, 133, 0) , -1)\n",
    "        \n",
    "#         font = cv.FONT_HERSHEY_SIMPLEX\n",
    "#         cv.putText(img,'OpenCV',(10,500), font, 4,(255,255,255),2,cv.LINE_AA)\n",
    "        out_img = cv2.putText(out_img , str(person.score) , (20,20) \n",
    "                             , cv2.FONT_HERSHEY_SIMPLEX ,0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        if direction is not None:\n",
    "            out_img = cv2.putText(out_img , direction , (20,40) \n",
    "                             , cv2.FONT_HERSHEY_SIMPLEX ,0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.LEFT_WRIST)] , keypoints[int(BodyPart.LEFT_ELBOW)])\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.LEFT_ELBOW)] , keypoints[int(BodyPart.LEFT_SHOULDER)])\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.LEFT_SHOULDER)] , keypoints[int(BodyPart.RIGHT_SHOULDER)])\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.RIGHT_SHOULDER)] , keypoints[int(BodyPart.RIGHT_ELBOW)])\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.RIGHT_ELBOW)] , keypoints[int(BodyPart.RIGHT_WRIST)])\n",
    "        \n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.LEFT_HIP)] , keypoints[int(BodyPart.LEFT_KNEE)])\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.LEFT_KNEE)] , keypoints[int(BodyPart.LEFT_ANKLE)])\n",
    "        \n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.RIGHT_HIP)] , keypoints[int(BodyPart.RIGHT_KNEE)])\n",
    "        out_img =  self.lineBetweenPoints(out_img , keypoints[int(BodyPart.RIGHT_KNEE)] , keypoints[int(BodyPart.RIGHT_ANKLE)])\n",
    "        \n",
    "        return out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now below we run a test on the model. Firstly we create a `Image` widget and then inside a forever loop run captured images through our model and display the results in the widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b8660c5a39428fb663a9158295118e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpg', height='353', width='257')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_widget = ipywidgets.Image(format='jpg' , height=HEIGHT ,width=WIDTH)\n",
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again to break from the loop below use `kernel -> interrupt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015070840181832851\n",
      "Breaking\n"
     ]
    }
   ],
   "source": [
    "stick_man = StickMan()\n",
    "try:\n",
    "    while True:\n",
    "        img = cam_read()[1]\n",
    "        if img is None:\n",
    "            break\n",
    "            \n",
    "        pil_img = Image.fromarray(img)\n",
    "        person = pnet.estimateSinglePose(pil_img)\n",
    "        #if person.bodyScore > 0.5:\n",
    "        img = stick_man.draw(pil_img,person)\n",
    "        \n",
    "        image_widget.value = cv2.imencode('.jpg',img)[1].tobytes()\n",
    "        clear_output(wait=True)\n",
    "        print(person.bodyScore)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Breaking\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Image Pipeline\n",
    "\n",
    "Having seen that we are repeating this process of creating a display widget and then running a loop. We encapsulate all of that inside a `ImagePipeline` class. We will use this class to quickly instantiate all the boilerplate stuff later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePipeline:\n",
    "    \n",
    "    def __init__(self ,camera , create_display=True , height=256 , width=256):\n",
    "        self.camera = camera\n",
    "        self.create_display = create_display\n",
    "        if self.create_display:\n",
    "            self.create_display_frame(height , width)\n",
    "    \n",
    "    def create_display_frame(self , height , width):\n",
    "        self.image_widget = ipywidgets.Image(format='jpg' , height=height ,width=width)\n",
    "        display(self.image_widget)\n",
    "    \n",
    "    def _process(self , img , **kwargs):\n",
    "        return img\n",
    "        \n",
    "    def run(self, **kwargs):\n",
    "        while True:\n",
    "            img = self.camera.read()[1]\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            if img is None:\n",
    "                break\n",
    "            img = self._process(img , **kwargs)\n",
    "            \n",
    "            if self.create_display:\n",
    "                self.image_widget.value = cv2.imencode('.jpg',img)[1].tobytes()\n",
    "                clear_output(wait=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Keypoints to Gesture\n",
    "\n",
    "Below we create a rule based system to identify 4 simple gestures. These gestures are simple enough to be distinguished easily by a bunch of `if-else` statements.\n",
    "\n",
    "|-|-|-|-|\n",
    "|----|----|---|----|\n",
    "|<img src=\"https://user-images.githubusercontent.com/6872080/118677163-092aaa80-b7ca-11eb-8f59-8d692e7c9148.png\" width=\"50%\">|<img src=\"https://user-images.githubusercontent.com/6872080/118677572-51e26380-b7ca-11eb-9968-24f2280e9197.png\" width=\"50%\" >|<img src=\"https://user-images.githubusercontent.com/6872080/118677377-38411c00-b7ca-11eb-9a85-d6ebdb8a7adc.png\" width=\"50%\">|<img src=\"https://user-images.githubusercontent.com/6872080/118677650-6161ac80-b7ca-11eb-9241-0cbfc0ea925e.png\" width=\"50%\">|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseToDirection:\n",
    "    \n",
    "    def x_dist(self,point_a , point_b):\n",
    "        return abs(point_a.position.x - point_b.position.x)\n",
    "    \n",
    "    def is_descend(self, points):\n",
    "        if np.all([points[c].score < 0.25 for c in [\"RIGHT_WRIST\" , \"LEFT_WRIST\" ]]):\n",
    "            return False\n",
    "        \n",
    "        if ((points[\"RIGHT_WRIST\"].position.x > points[\"RIGHT_SHOULDER\"].position.x)\n",
    "           and (points[\"LEFT_WRIST\"].position.x < points[\"LEFT_SHOULDER\"].position.x)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def is_left(self, points):\n",
    "        if np.all([points[c].score < 0.25 for c in [\"LEFT_WRIST\" , \"LEFT_SHOULDER\" ]]):\n",
    "            return False\n",
    "        if ((points[\"LEFT_WRIST\"].position.x < points[\"LEFT_SHOULDER\"].position.x)\n",
    "           and (points[\"LEFT_WRIST\"].position.y < points[\"LEFT_ELBOW\"].position.y)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def is_right(self, points):\n",
    "        if np.all([points[c].score < 0.25 for c in [\"RIGHT_WRIST\" , \"RIGHT_SHOULDER\" ]]):\n",
    "            return False\n",
    "        \n",
    "        if ((points[\"RIGHT_WRIST\"].position.x > points[\"RIGHT_SHOULDER\"].position.x)\n",
    "            and (points[\"RIGHT_WRIST\"].position.y < points[\"RIGHT_ELBOW\"].position.y)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def is_ascend(self, points):\n",
    "        if np.all([points[c].score < 0.25 for c in [\"RIGHT_WRIST\" , \"LEFT_WRIST\" ]]):\n",
    "            return False\n",
    "        \n",
    "        if ((points[\"RIGHT_WRIST\"].position.x < points[\"RIGHT_SHOULDER\"].position.x)\n",
    "           and (points[\"LEFT_WRIST\"].position.x > points[\"LEFT_SHOULDER\"].position.x)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def keypoints_to_direction(self , estimated_pose):\n",
    "        points = {}\n",
    "        for point in estimated_pose.keyPoints:\n",
    "            points[str(point.bodyPart).split(\".\")[1]] = point\n",
    "        \n",
    "        print(points)\n",
    "        if  (self.is_ascend(points)):\n",
    "            return \"ASCEND\"\n",
    "        elif (estimated_pose.score > 0.5 ) and self.is_descend(points):\n",
    "            return \"DESCEND\"\n",
    "        elif self.is_left(points):\n",
    "            return \"LEFT\"\n",
    "        elif self.is_right(points):\n",
    "            return \"RIGHT\"\n",
    "        else:\n",
    "            return \"NONE\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Server Architecture\n",
    "\n",
    "Now that we have code ready for going from image to gesture. We need to send these identified gestures to our laptop.\n",
    "For this we will use a `client-server` architecture.\n",
    "\n",
    "**Server:**\n",
    "Jetson Nano with the model running on top of it acting as a server and sends\n",
    "the identified gestures to the client.\n",
    "\n",
    "**Client:**\n",
    "A Laptop acting as a client with drone controller script and Mission Planner\n",
    "running on it. We will have a Drone controller script accept the stream of identified\n",
    "gestures and translated them into control commands for the drone. \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/6872080/118680434-bb637180-b7cc-11eb-9c77-155c17da5139.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessengerServer:\n",
    "    \n",
    "    def __init__(self , host=\"localhost\" , port=42425):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.s = socket.socket()\n",
    "        self.s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "        self.s.bind((host, port))\n",
    "        \n",
    "    \n",
    "    def wait_for_connection(self):\n",
    "        self.s.listen(1)\n",
    "        self.connection, addr = self.s.accept()\n",
    "    \n",
    "    def send(self , msg):\n",
    "        self.connection.send(msg.encode(\"utf-8\"))\n",
    "    \n",
    "    def close(self):\n",
    "        self.connection.close()\n",
    "        \n",
    "class MessengerClient:\n",
    "    def __init__(self , host=\"localhost\" , port=42425):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.s = socket.socket()\n",
    "    \n",
    "    def connect(self):\n",
    "        self.s.connect((self.host,self.port))\n",
    "    \n",
    "    def relay_msg(self , function):\n",
    "        while True:\n",
    "            data = s.recv(1024)\n",
    "            if data is None:\n",
    "                break\n",
    "            function(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communicating with the Client\n",
    "\n",
    "\n",
    "To send the identified direction to drone control we use a staggered/queue approach. We wait for some time and queue all the recognized gestures for that time. Then send the one with largest count to the `client`. Now once `client` recieves the gesture it will translate those to drone control signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectionToControl:\n",
    "    \n",
    "    def __init__(self , messenger_server , time_interval = 1):\n",
    "        self.trace = []\n",
    "        self.time_stamp = time.time()\n",
    "        self.messenger = messenger_server\n",
    "        self.time_interval = time_interval\n",
    "    \n",
    "    def find_max(self):\n",
    "        counts = defaultdict(int)\n",
    "        current_max = \"NONE\"\n",
    "        for i in self.trace:\n",
    "            counts[i] += 1\n",
    "            if counts[i] > counts[current_max]:\n",
    "                current_max = i\n",
    "        \n",
    "        return current_max\n",
    "            \n",
    "    def track_and_send(self, direction):\n",
    "        self.trace.append(direction)\n",
    "        print((time.time() - self.time_stamp) , self.time_interval)\n",
    "        if (time.time() - self.time_stamp) > self.time_interval:\n",
    "            print(\"Sending\")\n",
    "            self.messenger.send(self.find_max())\n",
    "            self.time_stamp = time.time()\n",
    "            self.trace = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Pipeline\n",
    "\n",
    "Below we integrate together all the pieces we have developed so-far. We are inheriting from the `ImagePipeline` class we created above and therefore will just override the `_process` method.\n",
    "\n",
    "The pipeline performs following steps\n",
    "\n",
    "1. Converted received image to `PIL` array\n",
    "2. Extract the keypoints using the `PoseEstimator`\n",
    "3. Classify the gesture based on extracted keypoints.\n",
    "4. Draw the gesture (if enabled using `create_display` attribute)\n",
    "5. Send the identified gesture to the `DirectionToControl` class, to be sent to the `client`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletePipeline(ImagePipeline):\n",
    "    \n",
    "    def __init__(self,vid_src , messenger_server , create_display=True , height=256 , width=256):\n",
    "        super().__init__(vid_src, create_display , height , width)\n",
    "        \n",
    "        #self.create_display = create_display\n",
    "        self.object_detector = ObjectDetector(vid_src , \"/tmp/detect.tflite\" , \"/tmp/coco_labels.txt\")\n",
    "        self.pose_estimator = Posenet(os.path.join(path , \"posenet_mobilenet_float_075_1_default_1.tflite\"))\n",
    "        self.direction_estimator = PoseToDirection()\n",
    "        self.direction_control = DirectionToControl(messenger_server)\n",
    "        self.stick_man = StickMan()\n",
    "        self.bound_tracker = BoundTracker()\n",
    "        self.itr = 0\n",
    "        self.trace = []\n",
    "    \n",
    "    def _process(self,image , threshold):\n",
    "        self.itr += 1\n",
    "        img = None\n",
    "\n",
    "        pil_img = Image.fromarray(image)\n",
    "        person = self.pose_estimator.estimateSinglePose(pil_img)\n",
    "        direction = self.direction_estimator.keypoints_to_direction(person)\n",
    "        \n",
    "        if self.create_display:\n",
    "            img = self.stick_man.draw(pil_img,person , direction)\n",
    "        \n",
    "        self.direction_control.track_and_send(direction)\n",
    "        return img\n",
    "            \n",
    "            \n",
    "    def dump(self , fname):\n",
    "        d = {\"fname\" : str(fname) , \"data\" : self.trace}\n",
    "        with open(fname ,\"w\") as fp:\n",
    "            json.dump(d,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 42425\n",
    "messenger_server = MessengerServer(port = port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now once you run the code below, the server script will wait for a client to connect to it. You should now switch to your client and run the client scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "messenger_server.wait_for_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP Messenger Client Before running code below\n",
    "\n",
    "Now that you have your client setup. Run the code below and pheww it's all integrated together. In your client script you should finally see series of `prints` of the gesture recognized. If you have hooked up mission planner via Mav Proxy you should be able to see drone movement in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a4ea14daab4608b306beb2126442d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpg', height='353', width='257')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmpipe = CompletePipeline(camera2 ,messenger_server=messenger_server, height=HEIGHT ,width=WIDTH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmpipe.run(threshold=0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'keyPoints': ['{\"bodyPart\": 0, \"position\": {\"x\": 118.0, \"y\": 26.0}, \"score\": 0.8567788626791317}',\n",
       "   '{\"bodyPart\": 1, \"position\": {\"x\": 125.0, \"y\": 17.0}, \"score\": 0.891833170924189}',\n",
       "   '{\"bodyPart\": 2, \"position\": {\"x\": 112.0, \"y\": 20.0}, \"score\": 0.8476605656514117}',\n",
       "   '{\"bodyPart\": 3, \"position\": {\"x\": 134.0, \"y\": 25.0}, \"score\": 0.590182399789272}',\n",
       "   '{\"bodyPart\": 4, \"position\": {\"x\": 104.0, \"y\": 28.0}, \"score\": 0.3752255137360918}',\n",
       "   '{\"bodyPart\": 5, \"position\": {\"x\": 137.0, \"y\": 80.0}, \"score\": 0.6957582805034676}',\n",
       "   '{\"bodyPart\": 6, \"position\": {\"x\": 99.0, \"y\": 79.0}, \"score\": 0.9458621737091122}',\n",
       "   '{\"bodyPart\": 7, \"position\": {\"x\": 143.0, \"y\": 118.0}, \"score\": 0.5022092943234557}',\n",
       "   '{\"bodyPart\": 8, \"position\": {\"x\": 93.0, \"y\": 116.0}, \"score\": 0.5801301498336171}',\n",
       "   '{\"bodyPart\": 9, \"position\": {\"x\": 137.0, \"y\": 137.0}, \"score\": 0.27683238004469657}',\n",
       "   '{\"bodyPart\": 10, \"position\": {\"x\": 83.0, \"y\": 195.0}, \"score\": 0.29424426551143823}',\n",
       "   '{\"bodyPart\": 11, \"position\": {\"x\": 131.0, \"y\": 153.0}, \"score\": 0.6990407198997565}',\n",
       "   '{\"bodyPart\": 12, \"position\": {\"x\": 103.0, \"y\": 154.0}, \"score\": 0.6707052782770424}',\n",
       "   '{\"bodyPart\": 13, \"position\": {\"x\": 113.0, \"y\": 281.0}, \"score\": 0.38530579248827584}',\n",
       "   '{\"bodyPart\": 14, \"position\": {\"x\": 104.0, \"y\": 279.0}, \"score\": 0.37570845560440425}',\n",
       "   '{\"bodyPart\": 15, \"position\": {\"x\": 115.0, \"y\": 338.0}, \"score\": 0.1562945392246266}',\n",
       "   '{\"bodyPart\": 16, \"position\": {\"x\": 108.0, \"y\": 339.0}, \"score\": 0.2108309384091551}'],\n",
       "  'score': 0.550270751800538,\n",
       "  'itr': 41}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmpipe.trace"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
